The SHIELD Scheduler
====================

Starting with version 8.1, SHIELD features a completely rewritten
scheduler based on a fairer scheduling algorithm.  The SHIELD Core
API provides more visibility into the state of the scheduler, a
behavior we as SHIELD operators sorely missed from previous
versions.

Tasks vs. Chores
----------------

The new scheduler operates entirely in memory, on objects called
_chores_.  A _chore_ encapsulates the computation function itself,
as well as accessories that enable the execution and output
handling we expect from SHIELD tasks.

The `scheduler.Chore` object, defined in
`$src/core/scheduler/chore.go`, contains the following relevant
fields:

  - **ID** - A unique identifier for this chore, in both time and
    space.

  - **TaskUUID** - The UUID of the database-resident Task object
    (more on this later).

  - **Do** - A [thunk][thunk] that provides the computation to
    perform when this Chore is executed.  This is provided by a
    [Fabric][fabric].

The SHIELD scheduler deals exclusively in chores.

What then, is a _task_?

A _task_ is a database-resident record of a chore, future or
otherwise.  All the other components of SHIELD communicate to the
scheduler via the database, by inserting `pending` tasks.

The conversion of Tasks into Chores occurs inside of the SHIELD
Core main loop, defined in `$src/core2/main.go`.  The
`TasksToChores()` method, called from fast loop, is responsible
for taking each pending task and submitting its corresponding
_chore_ objects to the scheduler.



The Scheduling Algorithm
------------------------

The scheduler accepts submitted chores and places them into a
priority queue.  This is implemented as an array of lists.  The
outer array is indexed by priority; the inner list is ordered by
time of submission, FIFO-style.

A semi-graphical representation of the priority queue might look
like this:

    [0] (empty)
    [1] -> { BACKUP 'Some Database System' TO 'S3' }
           { BACKUP 'Web Files' TO 'S3' }
    [2] (empty)
    [3] -> { TEST-STORE 'S3' }
    [4] -> { PURGE 'fb182c4c-7031' FROM 'S3' }

Priority values range from 0 to 99.  Lower priority values are
higher priority chores.  Put another way, the closer to zero (0)
the priority, the sooner that chore will be executed.

SHIELD assigns initial priorities as follows:

| Priority | Purpose |
| -------- | ------- |
| 0        | _Ad hoc_ backup, restore, purge, and test-store operations. |
| 10       | _Ad hoc_ agent-status operations. |
| 20       | Scheduled backup operations. |
| 30       | Scheduled test-store operations. |
| 40       | Scheduled agent-status operations. |
| 50       | Scheduled archive purge operations. |

The strategy here is to prioritize _interactive_ operations like
ad hoc backup operations over scheduled operations; we would
rather SHIELD not hold up the operator in favor of its own
internal operations.

Each type of scheduled operation is assigned its own priorities,
so that metadata and cleanup operations (like archive purge or
storage testing) do not "starve out" the backup operations.
SHIELD is, after all, a data protection solution, and non-backup
operations can be delayed with minimal fallout.

The scheduler algorithm traverses this priority queue, exhausting
each sub-list (all chores of a given priority) before moving onto
the next.  As soon as the scheduler runs out of scheduling threads
to execute chores in, it stops.

The Elevator Algorithm
----------------------

The priority queue implementation ensures that the scheduler is
fairer to the needs of users, by prioritizing certain task types
over others.  A naÃ¯ve implementation leads to some nasty
starvation cases wherein a flurry of high-priority chores can keep
the scheduling threads so busy that lower-priority chores never
get run.  Since backup chores are mid-level priority (30), this
could prove disastrous.

To compensate, the scheduler implements an _elevator algorithm_
that gradually increases the priority of chores in an attempt to
lend precedence to older, lower-priority chores.

Assume the following priority queue:

    [5] -> { ... 15 long-running chores ... }
    [7] -> { BACKUP 'Scheduled System' TO 'Cloud' }
    [9] -> { TEST-STORE 'Cloud' }

At regular intervals, the scheduler _elevates_:

  1. All 0-priority chores are saved to a temporary list

  2. Each lower-priority list is moved up a slot.

  3. The saved (previous) 0-priority chores are appended to the
     end of the new 0-priority chores.


Using the above scheduler state, running the elevator algorithm
leads to the following actions being taken:

  1. The (empty) priority 0 is saved to a temporary list.

  2. The chores at priority 5 are moved to priority 0.

  3. The BACKUP chore at priority 7 is moved to priority 5.

  4. The TEST-STORE chore at priority 9 is moved to priority 7.


This yields the following priority queue:

    [0] -> { ... 15 long-running chores ... }
    [5] -> { BACKUP 'Scheduled System' TO 'Cloud' }
    [7] -> { TEST-STORE 'Cloud' }


This does not substantially changed the state of the scheduler;
all chores have simply moved up in priority.

On the _next_ run of the elevator, algorithm, however, things
start to get interesting:

  1. The 15 long-running chores at priority 0 are saved off.

  2. The BACKUP chore at priority 5 is moved to priority 0.

  3. The TEST-STORE chore at priority 7 is moved to priority 5.

  4. The 15 long-running chores that used to be at priority 0
     are appended to the new priority 0 chore list.


Our priority queue now looks like this:


    [0] -> { BACKUP 'Scheduled System' TO 'Cloud' }
           { ... 15 long-running chores ... }
    [5] -> { TEST-STORE 'Cloud' }

And the scheduler will now prioritize the BACKUP chore over the
long-running chores that would otherwise starve out the scheduler.

Note that the intervening (empty) priorities are ignored.  Doing
so speeds up the elevation re-prioritization process, and avoids
situations where constant submission of higher priority chores can
still starve out the scheduler.

Consider the above scenario, except that 4 new long-running chores
are submitted between each scheduler run / elevation.  We start
back at the beginning with this priority queue:

    [5] -> { ... 15 long-running chores ... }
    [7] -> { BACKUP 'Scheduled System' TO 'Cloud' }
    [9] -> { TEST-STORE 'Cloud' }

After our first elevation, we have:

    [0] -> { ... 15 long-running chores ... }
    [5] -> { BACKUP 'Scheduled System' TO 'Cloud' }
           { ... 4 new long-running chores ... }
    [7] -> { TEST-STORE 'Cloud' }

Note that the BACKUP task now at priority 5 takes precedence over
the new priority 5 chores that got submitted after the scheduler
elevated.

A subsequent elevation / chore submission roung looks like this:

    [0] -> { BACKUP 'Scheduled System' TO 'Cloud' }
           { ... 4 new long-running chores ... }
           { ... 15 long-running chores ... }
    [5] -> { TEST-STORE 'Cloud' }
           { ... 4 more new long-running chores ... }

Again, the TEST-STORE that was re-prioritized to priority 5 takes
precedence of the next 4 long-running chores that get submitted.
Eventually, despite the breakneck proliferation of these
long-running, high-priority chores, the lower-priority work will
get scheduled.

The elevator algorithm is implemented in
`$src/core/scheduler/elevator.go`.  Note that the SHIELD Core runs
the scheduler's scheduling algorithm in the _hyper_ loop, but runs
the elevator algorithm in the _fast_ loop; this is by design.


Internal Tasks
--------------

Sometimes, there is a bit of work that we need the SHIELD core to
perform, that we would like for SHIELD administrators to be able
to review.  Tasks are a great fit for this, except that the work
doesn't always require an agent.

A prime example of this is the storage analysis work that needs to
be done regularly, for deltas and forecasting.  There is stepwise
information that can be invaluable in debugging or troubleshooting
usage / forecasting data, or determining _where_ space in a shared
(global) cloud store is going.

To accomplish this, SHIELD has the concept of an _internal task_,
which leverages the task+chore apparatus to execute, but bypasses
the fabrics and their connected agents.

When an internal task is "scheduled", a Task record gets put into
the database by calling CreateInternalTask, like this:

    t, err := db.CreateInternalTask(owner, op, tenantUUID)

This new task will be put into the database in the _running_ state
(`db.RunningStatus`), so that the scheduling process skips it.
This means that it is up to you, the caller, to schedule the
chore, and provide the chore function:

    s.Schedule(priority, scheduler.NewChore(
      t.UUID, /* our task, returned from the database call */
      func (chore scheduler.Chore) {
        // do the work here.
      }
    )

This lets us use the chore framework to write the work function,
and provides a log (via the "output" of the chore) of what was
done, all in a package convenient and familiar to SHIELD
operators.


[thunk]:  https://en.wikipedia.org/wiki/Thunk
[fabric]: $docs/dev/fabric.md
